{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e7cf683-9c23-42b3-b37d-310f2bd7b913",
   "metadata": {},
   "source": [
    "# Supervised Fine Tuning (SFT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8911276a-22cf-4849-9607-d5d2437501ac",
   "metadata": {},
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6946b872-aaa1-44c0-994c-440a68df4b9a",
   "metadata": {},
   "source": [
    "Supervised Fine-Tuning (SFT) is one of the most straightforward ways to improve the performance of large language models. Unlike preference-based methods like DPO, SFT works when we already have high-quality, labeled data‚Äîwhere each input is paired with a desired output. This makes it ideal for tasks where ground truth responses are available, such as summarization, question answering, or instruction following."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6e0820-7677-40c9-b681-b5cba071aece",
   "metadata": {},
   "source": [
    "## How it works (High Level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ab3711-c73b-4e80-8d71-6d21d87b83e2",
   "metadata": {},
   "source": [
    "In SFT, we train the model to mimic the desired behavior by directly optimizing for correct outputs using standard supervised learning. The model sees input-output pairs and learns to produce the expected result. This method is simple and effective, especially when the training data is well-aligned with the task you want the model to perform. It‚Äôs often used as a first step before applying more advanced techniques like reinforcement learning or preference optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57aa7956-a6e6-4380-8354-6d20289d9b6d",
   "metadata": {},
   "source": [
    "## How it really works (Teacher Forcing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67031e0f-02bd-4086-b331-b9038d030516",
   "metadata": {},
   "source": [
    "SFT also uses **Teacher Forcing** during training. This means we feed the correct output (the target sequence) to the model, one token at a time, and calculate how likely the model is to generate each token. The training objective is to increase the probability of generating the correct tokens, step by step.\n",
    "\n",
    "By learning directly from labeled examples, the model becomes better at producing responses that match the expected outputs. There's no comparison between alternatives like in DPO‚Äîjust direct learning from ground truth sequences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c004f27b-cbb1-4e5f-bd28-c12ab82276c0",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c471d41-1358-42a0-bc71-65f7c7975add",
   "metadata": {},
   "source": [
    "To use SFT, you need a dataset of **(input, output)** pairs. Each example should include a prompt or instruction and the correct response. These outputs are usually written by humans or curated carefully to reflect the desired behavior. The quality of this data is very important‚Äîbetter data leads to better model performance. Unlike DPO, there's no need for preference comparisons or rejected alternatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71d1ff7-22ec-450a-969c-28b1fd9c42a5",
   "metadata": {},
   "source": [
    "**Important Note: Use the notebook [Dataset-Creation-SFT.ipynb](Dataset-Creation-SFT.ipynb) to prepare DPO dataset before running this notebook.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d0682f-c29e-4199-9d8f-000e59314ac3",
   "metadata": {},
   "source": [
    "## Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb21287-6b35-4656-a0c1-dfa5ddca26aa",
   "metadata": {},
   "source": [
    "Training or running large language models often requires significant memory, which can be a challenge on limited hardware. **Quantization** helps by reducing the size of the model weights, typically from 16 or 32-bit floats down to 8-bit or even 4-bit integers. This makes the model smaller and faster, with minimal impact on performance in most cases. In this project, we use the **BitsAndBytes** library to apply quantization efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf05561-bd69-4547-93ab-71c8b8179069",
   "metadata": {},
   "source": [
    "## Parameter Efficient Fine Tuning (PEFT) with Low Rank Adaptation (LoRA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922c235d-45da-4f6f-9271-1354a8d45646",
   "metadata": {},
   "source": [
    "Fine-tuning large models from scratch can be expensive and slow. **PEFT** techniques aim to reduce this cost by updating only a small number of parameters. One popular method is **Low Rank Adaptation (LoRA)**, which injects small trainable matrices into the model's layers without changing the original weights. This allows for efficient fine-tuning with fewer resources. We use the **peft** library to implement LoRA in our experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b77278-98f6-41bf-81bd-75c7c418f2f7",
   "metadata": {},
   "source": [
    "## Signal to Noise Ratio (SNR) with Spectrum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48069f5d-fe1a-41c2-9992-386331aeb3ee",
   "metadata": {},
   "source": [
    "Not all layers in a model contribute equally to learning during fine-tuning. **Signal to Noise Ratio (SNR)** helps identify which layers are more useful to focus on by comparing meaningful signal to background noise in the weight updates. This can guide efficient adaptation and avoid overfitting. We use the **spectrum** library to compute and analyze SNR during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bf003b-c54e-4357-9fab-dde99da60575",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2486748d-4bb1-4727-a7bf-544ba62b6f01",
   "metadata": {},
   "source": [
    "To evaluate our fine-tuned language model, we use several metrics:\n",
    "\n",
    "- **accuracy**: Measures how often the model's output matches the expected output exactly.\n",
    "- **bleu**: A precision-based metric that compares n-gram overlap between generated and reference texts, commonly used in translation tasks.\n",
    "- **rouge1, rouge2, rougeL**: Recall-based metrics that check how many unigrams (rouge1), bigrams (rouge2), or longest common sequences (rougeL) overlap with the target.\n",
    "- **bertscore_precision, bertscore_recall, bertscore_f1**: Use BERT embeddings to compare the similarity of generated and reference sentences on a deeper, semantic level.\n",
    "- **avg_levenshtein**: Measures the average number of edits (insertions, deletions, substitutions) needed to change the model output into the reference text, useful for judging closeness in form.\n",
    "\n",
    "Together, these metrics give a balanced view of both surface-level accuracy and deeper semantic alignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9fe549-3c68-476e-9374-a0879339ab12",
   "metadata": {},
   "source": [
    "## Note on Runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a60ccca-244a-4c60-886b-fa30dbb5178a",
   "metadata": {},
   "source": [
    "Due to limited compute and time, the experiments and metrics in this guide were run in under 2 hours. In practice, fine-tuning large models usually requires multiple days or even weeks to achieve optimal results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92fbb15-80bb-4a3c-9d63-ab55ccaaaf67",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bed9d6-941b-42c0-aded-16b6c30cee58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "import yaml\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from datasets import load_dataset, DatasetDict, Dataset\n",
    "import Levenshtein\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import json\n",
    "from transformers import set_seed\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae51ea51-8172-4eac-9b69-c56d27e8f64a",
   "metadata": {},
   "source": [
    "# Print Versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e7e3d5b-12d6-4c62-bce1-fafe293cf3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üå± Welcome to your Environment Info Report! üå±\n",
      "\n",
      "üêç Python:       CPython 3.12.7\n",
      "üíª Platform:     Linux 6.11.0-26-generic (x86_64)\n",
      "\n",
      "üöÄ CUDA Available: True\n",
      "   ‚Ä¢ CUDA Version:   12.4\n",
      "   ‚Ä¢ cuDNN Version:  90100\n",
      "   ‚Ä¢ GPU Count:      1\n",
      "     - GPU 0:      NVIDIA GeForce RTX 4070 Laptop GPU\n",
      "\n",
      "üì¶ nvcc: Cuda compilation tools, release 12.1, V12.1.105\n",
      "\n",
      "üìä nvidia-smi info:\n",
      "   NVIDIA GeForce RTX 4070 Laptop GPU, 570.133.07, 8188 MiB\n",
      "\n",
      "üìö Library versions:\n",
      "üîñ torch           version: 2.6.0+cu124\n",
      "üîñ torchvision     version: 0.21.0+cu124\n",
      "üîñ torchaudio      version: 2.6.0+cu124\n",
      "üîñ transformers    version: 4.51.3\n",
      "üîñ accelerate      version: 1.6.0\n",
      "üîñ trl             version: 0.17.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/electron/PycharmProjects/fine_tune_llm/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîñ peft            version: 0.15.2\n",
      "üîñ deepspeed       not installed\n",
      "üîñ bitsandbytes    version: 0.45.5\n",
      "üîñ datasets        version: 3.6.0\n",
      "üîñ evaluate        version: 0.4.3\n",
      "üîñ tokenizers      version: 0.21.1\n",
      "üîñ sentencepiece   version: 0.2.0\n",
      "üîñ huggingface_hub version: 0.31.2\n",
      "üîñ numpy           version: 2.2.5\n",
      "üîñ scipy           version: 1.15.3\n",
      "üîñ pandas          version: 2.2.3\n",
      "üîñ scikit-learn    version: 1.6.1\n",
      "üîñ wandb           version: 0.19.11\n",
      "üîñ tensorboard     not installed\n",
      "üîñ mlflow          not installed\n",
      "\n",
      "üîß Conda env: (none)\n",
      "\n",
      "üåê Environment variables:\n",
      "   - CUDA_HOME       = \n",
      "   - CUDA_PATH       = \n",
      "   - LD_LIBRARY_PATH = \n",
      "   - HF_HOME         = \n",
      "   - HF_DATASETS_CACHE = \n",
      "\n",
      "‚ú® All set! Keep growing and training with confidence! ‚ú®\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import platform\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "\n",
    "def print_env_info():\n",
    "    print(\"\\nüå± Welcome to your Environment Info Report! üå±\\n\")\n",
    "\n",
    "    # Python & OS\n",
    "    print(f\"üêç Python:       {platform.python_implementation()} {platform.python_version()}\")\n",
    "    print(f\"üíª Platform:     {platform.system()} {platform.release()} ({platform.machine()})\\n\")\n",
    "\n",
    "    # CUDA & GPU via PyTorch\n",
    "    try:\n",
    "        import torch\n",
    "        cuda_avail = torch.cuda.is_available()\n",
    "        print(f\"üöÄ CUDA Available: {cuda_avail}\")\n",
    "        if cuda_avail:\n",
    "            print(f\"   ‚Ä¢ CUDA Version:   {torch.version.cuda}\")\n",
    "            print(f\"   ‚Ä¢ cuDNN Version:  {torch.backends.cudnn.version()}\")\n",
    "            n_gpus = torch.cuda.device_count()\n",
    "            print(f\"   ‚Ä¢ GPU Count:      {n_gpus}\")\n",
    "            for i in range(n_gpus):\n",
    "                print(f\"     - GPU {i}:      {torch.cuda.get_device_name(i)}\")\n",
    "    except ImportError:\n",
    "        print(\"üö´ PyTorch not installed, skipping CUDA/GPU info\")\n",
    "\n",
    "    # nvcc (if available)\n",
    "    try:\n",
    "        out = subprocess.check_output(['nvcc', '--version'], stderr=subprocess.STDOUT)\n",
    "        release = [l for l in out.decode().splitlines() if \"release\" in l]\n",
    "        print(f\"\\nüì¶ nvcc: {release[-1].strip()}\")\n",
    "    except Exception:\n",
    "        print(\"\\nüì¶ nvcc: not found in PATH\")\n",
    "\n",
    "    # nvidia-smi\n",
    "    try:\n",
    "        out = subprocess.check_output([\n",
    "            'nvidia-smi',\n",
    "            '--query-gpu=name,driver_version,memory.total',\n",
    "            '--format=csv,noheader'], stderr=subprocess.DEVNULL\n",
    "        ).decode().strip().splitlines()\n",
    "        print(\"\\nüìä nvidia-smi info:\")\n",
    "        for line in out:\n",
    "            print(\"   \" + line)\n",
    "    except Exception:\n",
    "        print(\"\\nüìä nvidia-smi: not available\")\n",
    "\n",
    "    # Helper to show versions\n",
    "    def show_ver(label, import_name=None):\n",
    "        try:\n",
    "            m = __import__(import_name or label)\n",
    "            v = getattr(m, '__version__', None) or getattr(m, 'VERSION', None) or str(m)\n",
    "            print(f\"üîñ {label:<15} version: {v}\")\n",
    "        except ImportError:\n",
    "            print(f\"üîñ {label:<15} not installed\")\n",
    "\n",
    "    # Popular ML/LLM libraries\n",
    "    print(\"\\nüìö Library versions:\")\n",
    "    libs = [\n",
    "        ('torch',      None),\n",
    "        ('torchvision', None),\n",
    "        ('torchaudio',  None),\n",
    "        ('transformers', None),\n",
    "        ('accelerate',  None),\n",
    "        ('trl',         'trl'),\n",
    "        ('peft',        'peft'),\n",
    "        ('deepspeed',   None),\n",
    "        ('bitsandbytes', None),\n",
    "        ('datasets',    'datasets'),\n",
    "        ('evaluate',    'evaluate'),\n",
    "        ('tokenizers',  None),\n",
    "        ('sentencepiece', None),\n",
    "        ('huggingface_hub', None),\n",
    "        ('numpy',       'numpy'),\n",
    "        ('scipy',       'scipy'),\n",
    "        ('pandas',      'pandas'),\n",
    "        ('scikit-learn','sklearn'),\n",
    "        ('wandb',       'wandb'),\n",
    "        ('tensorboard', 'tensorboard'),\n",
    "        ('mlflow',      'mlflow'),\n",
    "    ]\n",
    "    for label, name in libs:\n",
    "        show_ver(label, name)\n",
    "\n",
    "    # Conda env and key env vars\n",
    "    print(\"\\nüîß Conda env:\", os.getenv('CONDA_DEFAULT_ENV', '(none)'))\n",
    "    important_vars = ['CUDA_HOME', 'CUDA_PATH', 'LD_LIBRARY_PATH', 'HF_HOME', 'HF_DATASETS_CACHE']\n",
    "    print(\"\\nüåê Environment variables:\")\n",
    "    for var in important_vars:\n",
    "        print(f\"   - {var:<15} = {os.getenv(var, '')}\")\n",
    "\n",
    "    print(\"\\n‚ú® All set! Keep growing and training with confidence! ‚ú®\\n\")\n",
    "\n",
    "print_env_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0e58e4-882c-4c1a-afa2-684f37b88784",
   "metadata": {},
   "source": [
    "# Seed and Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dd6cdf2-acac-4b09-b9e2-c5dd0d8910fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "WANDB_TOKEN = \"WANDB-TOKEN-GOES-HERE\"\n",
    "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "SEED = 69"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7c1065-1455-4eb9-9c34-18a9c4b0304b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_API_KEY\"] = WANDB_TOKEN\n",
    "os.environ[\"WANDB_PROJECT\"] = \"lora_fine_tune_math_question_answer_spectrum\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1961cde7-4f60-44c0-8261-078eba8acce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int = SEED):\n",
    "    # Python built-in\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Numpy\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # PyTorch\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True  # Slower but reproducible\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # Environment variables\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "    # Hugging Face Transformers\n",
    "    set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66eeb5d8-566b-4774-8ed0-8d18ce6668cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e4e45c-533f-44e0-bcde-32315f654825",
   "metadata": {},
   "source": [
    "# Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c4eaa5d-ecf1-40a4-8264-9c8c85b52b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_quantization_4bit(\n",
    "    quant_type: str = \"nf4\",\n",
    "    use_double_quant: bool = True,\n",
    "    compute_dtype=torch.float16,\n",
    ") -> BitsAndBytesConfig:\n",
    "    \"\"\"\n",
    "    Create a BitsAndBytesConfig for 4-bit quantization.\n",
    "    \"\"\"\n",
    "    return BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=quant_type,\n",
    "        bnb_4bit_use_double_quant=use_double_quant,\n",
    "        bnb_4bit_compute_dtype=compute_dtype,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f948de75-08ee-477c-93f0-375936adbb28",
   "metadata": {},
   "source": [
    "# Load Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "517857de-3888-4b66-8266-ae6837c2d843",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer(\n",
    "    model_name: str,\n",
    "    bnb_config: BitsAndBytesConfig,\n",
    "    device_map: str = \"auto\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Load the pretrained model in 4-bit and its tokenizer.\n",
    "    Adjust tokenizer/model configs for chat formatting and training.\n",
    "    \"\"\"\n",
    "    # Tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    # Ensure we have a pad token\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.add_special_tokens({\"pad_token\": tokenizer.eos_token})\n",
    "    # Model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=device_map,\n",
    "    )\n",
    "    # Required for 4-bit finetuning\n",
    "    model.config.use_cache = False\n",
    "    # Resize token embeddings if we added a pad token\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d95f25-f32c-4de9-8269-8e3ee7473c4a",
   "metadata": {},
   "source": [
    "# PEFT (LoRA) + Spectrum (SNR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55a215c8-b795-46ca-b7b0-b6af79fef47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lora_peft(\n",
    "    model: torch.nn.Module,\n",
    "    r: int = 16,\n",
    "    alpha: int = 32,\n",
    "    dropout: float = 0.05,\n",
    "    target_modules: list = [\"q_proj\", \"v_proj\"],\n",
    "    spectrum_yaml_path: str = \"/home/electron/PycharmProjects/fine_tune_llm/spectrum/\"\n",
    "                    \"snr_results_TinyLlama-TinyLlama-v1.1_unfrozenparameters_10percent.yaml\",\n",
    "    spectrum: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Wrap the base model with PEFT + LoRA adapters.\n",
    "    \"\"\"\n",
    "    if spectrum:\n",
    "        MODEL_NAME = \"TinyLlama/TinyLlama_v1.1\"\n",
    "        \n",
    "        # 1) Load the YAML\n",
    "        if not os.path.isfile(spectrum_yaml_path):\n",
    "            raise FileNotFoundError(\n",
    "                f\"Spectrum YAML not found at {spectrum_yaml_path}. \"\n",
    "                \"Please run Spectrum to generate it first.\"\n",
    "            )\n",
    "        with open(spectrum_yaml_path, \"r\") as yf:\n",
    "            data = yaml.safe_load(yf)\n",
    "    \n",
    "        # 2) Extract the regex list\n",
    "        target_modules = data.get(\"unfrozen_parameters\")\n",
    "        if not target_modules or not isinstance(target_modules, list):\n",
    "            raise ValueError(\n",
    "                f\"No 'unfrozen_parameters' list found in {spectrum_yaml_path}.\"\n",
    "            )\n",
    "    \n",
    "    print('Target Lora Modules: ', target_modules)\n",
    "    \n",
    "    lora_cfg = LoraConfig(\n",
    "        r=r,\n",
    "        lora_alpha=alpha,\n",
    "        target_modules=target_modules,\n",
    "        lora_dropout=dropout,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    return get_peft_model(model, lora_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8088774-181b-4710-954d-52e7df1905bd",
   "metadata": {},
   "source": [
    "# SFT Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8eee42b-dd15-48c4-9d83-68740977ef9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_training_args(\n",
    "    output_dir: str = \"sft_tinnyllama\",\n",
    "    learning_rate: float = 1e-4,\n",
    "    per_device_train_batch_size: int = 2,\n",
    "    per_device_eval_batch_size: int = 2,\n",
    "    num_train_epochs: int = 1,\n",
    "    logging_steps: int = 20,\n",
    "    eval_steps: int = 200,\n",
    "    save_steps: int = 200,\n",
    "    gradient_accumulation_steps: int = 2,\n",
    "    bf16: bool = False,\n",
    "    fp16: bool = True,\n",
    "    report_to: str = \"wandb\",\n",
    "    run_name: str = \"tinyllama-sft\",\n",
    "    packing: bool = True,\n",
    "):\n",
    "    return SFTConfig(\n",
    "        output_dir=output_dir,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        logging_steps=logging_steps,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=eval_steps,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=save_steps,\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        bf16=bf16,\n",
    "        fp16=fp16,\n",
    "        optim=\"paged_adamw_32bit\",\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        warmup_ratio=0.03,\n",
    "        load_best_model_at_end=True,\n",
    "        greater_is_better=False,               # minimize eval loss\n",
    "        report_to=report_to,\n",
    "        run_name=run_name,\n",
    "        packing=packing,\n",
    "        save_total_limit=3,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3c4554-d553-42d5-9dd3-f5eff8354a84",
   "metadata": {},
   "source": [
    "# SFT Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0235923a-6ea3-4adc-89c0-21d5eb74b946",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_trainer(\n",
    "    model: torch.nn.Module,\n",
    "    tokenizer,\n",
    "    train_dataset,\n",
    "    eval_dataset,\n",
    "    training_args: SFTConfig,\n",
    ") -> SFTTrainer:\n",
    "    \"\"\"\n",
    "    Instantiate the TRL SFTTrainer.\n",
    "    \"\"\"\n",
    "    return SFTTrainer(\n",
    "        model=model,\n",
    "        processing_class=tokenizer,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a2e2d7-0f4f-4fff-ba9f-b8fff1549da5",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5eda34-0780-4483-a494-46e36d1d59bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(\n",
    "    model: torch.nn.Module,\n",
    "    tokenizer,\n",
    "    dataset: Dataset,\n",
    "    max_length: int = 256,\n",
    "    batch_size: int = 16,\n",
    "    device: str = None,\n",
    ") -> Dataset:\n",
    "    device = device or (next(model.parameters()).device)\n",
    "    model.eval()\n",
    "\n",
    "    def gen_batch(batch):\n",
    "        prompts = [\n",
    "            \"\".join(m[\"content\"] for m in msgs if m[\"role\"] in (\"system\", \"user\"))\n",
    "            for msgs in batch[\"messages\"]\n",
    "        ]\n",
    "        enc = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model.generate(\n",
    "                **enc,\n",
    "                max_new_tokens=max_length,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "        batch[\"prediction\"] = tokenizer.batch_decode(out[:, enc[\"input_ids\"].shape[-1]:], skip_special_tokens=True)\n",
    "        return batch\n",
    "\n",
    "    return dataset.map(gen_batch, batched=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f8eee8-3349-46c3-9d01-c9b51b5a87a7",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb5f406-60a2-4a1d-806e-ae5ba66e1c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_generation(\n",
    "    dataset_with_preds,\n",
    "    reference_col: str = \"messages\",\n",
    "    prediction_col: str = \"prediction\",\n",
    ") -> dict:\n",
    "    # Extract references\n",
    "    refs = [\n",
    "        \"\".join(m[\"content\"] for m in sample[reference_col] if m[\"role\"] == \"assistant\")\n",
    "        for sample in dataset_with_preds\n",
    "    ]\n",
    "    preds = dataset_with_preds[prediction_col]\n",
    "\n",
    "    # 1) Exact-match accuracy\n",
    "    matches = [int(p.strip() == r.strip()) for p, r in zip(preds, refs)]\n",
    "    accuracy = sum(matches) / len(matches)\n",
    "\n",
    "    # 2) BLEU, ROUGE, BERTScore\n",
    "    bleu_metric = evaluate.load(\"bleu\")\n",
    "    rouge_metric = evaluate.load(\"rouge\")\n",
    "    bert_metric = evaluate.load(\"bertscore\")\n",
    "\n",
    "    bleu = bleu_metric.compute(predictions=preds, references=[[r] for r in refs])[\"bleu\"]\n",
    "    rouge_scores = rouge_metric.compute(predictions=preds, references=refs)\n",
    "    bert_scores = bert_metric.compute(predictions=preds, references=refs, lang=\"en\")\n",
    "\n",
    "    # 3) Average Levenshtein distance\n",
    "    distances = [Levenshtein.distance(p, r) for p, r in zip(preds, refs)]\n",
    "    avg_levenshtein = float(np.mean(distances))\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"bleu\": bleu,\n",
    "        \"rouge1\": rouge_scores[\"rouge1\"],\n",
    "        \"rouge2\": rouge_scores[\"rouge2\"],\n",
    "        \"rougeL\": rouge_scores[\"rougeL\"],\n",
    "        \"bertscore_precision\": float(np.mean(bert_scores[\"precision\"])),\n",
    "        \"bertscore_recall\": float(np.mean(bert_scores[\"recall\"])),\n",
    "        \"bertscore_f1\": float(np.mean(bert_scores[\"f1\"])),\n",
    "        \"avg_levenshtein\": avg_levenshtein,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becc2c25-fe8b-479b-a58d-f9bb60111fae",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31a2d513-1fba-49d8-b76a-0db7fa941dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_for_sft(data_dir: str = \"./lora_processed_data\") -> DatasetDict:\n",
    "    files = {\n",
    "        \"train\": os.path.join(data_dir, \"train.jsonl\"),\n",
    "        \"eval\": os.path.join(data_dir, \"eval.jsonl\"),\n",
    "        \"test\": os.path.join(data_dir, \"test.jsonl\")\n",
    "    }\n",
    "    ds = load_dataset(\"json\", data_files=files)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f157361b-42ea-4412-98ac-a35fb14f5efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['messages'],\n",
       "        num_rows: 160028\n",
       "    })\n",
       "    eval: Dataset({\n",
       "        features: ['messages'],\n",
       "        num_rows: 20003\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['messages'],\n",
       "        num_rows: 20004\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sft_ds = load_for_sft(\"lora_processed_data\")\n",
    "sft_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "154dd9b7-dc92-4c46-af5f-35ebc8b4e792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['messages'],\n",
       "        num_rows: 10000\n",
       "    })\n",
       "    eval: Dataset({\n",
       "        features: ['messages'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['messages'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sft_ds['train'] = sft_ds['train'].shuffle(seed=SEED).select(range(10000))\n",
    "sft_ds['eval'] = sft_ds['eval'].shuffle(seed=SEED).select(range(1000))\n",
    "sft_ds['test'] = sft_ds['test'].shuffle(seed=SEED).select(range(1000))\n",
    "sft_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6be73fbc-56d6-47df-93d7-d642b7b9efc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'Solve the given high school math problem by providing a clear explanation of each step leading to the final solution.\\n\\n    Provide a detailed breakdown of your calculations, beginning with an explanation of the problem and describing how you derive each formula, value, or conclusion. Use logical steps that build upon one another, to arrive at the final answer in a systematic manner.\\n\\n    # Steps\\n\\n    1. **Understand the Problem**: Restate the given math problem and clearly identify the main question and any important given values.\\n    2. **Set Up**: Identify the key formulas or concepts that could help solve the problem (e.g., algebraic manipulation, geometry formulas, trigonometric identities).\\n    3. **Solve Step-by-Step**: Iteratively progress through each step of the math problem, justifying why each consecutive operation brings you closer to the solution.\\n    4. **Double Check**: If applicable, double check the work for accuracy and sense, and mention potential alternative approaches if any.\\n    5. **Final Answer**: Provide the numerical or algebraic solution clearly, accompanied by appropriate units if relevant.\\n\\n    # Notes\\n\\n    - Always clearly define any variable or term used.\\n    - Wherever applicable, include unit conversions or context to explain why each formula or step has been chosen.\\n    - Assume the level of mathematics is suitable for high school, and avoid overly advanced math techniques unless they are common at that level.\\n    '},\n",
       " {'role': 'user',\n",
       "  'content': 'On Stewar Farm, the ratio between the number of sheep and the number of horses is 5 to 7. If each horse is fed 230 ounces of horse food per day and the farm needs a total of 12880 ounces of horse food per day, find the number of sheep on the farm. Additionally, each sheep consumes 150 ounces of sheep food per day, and the farm has a total daily food expenditure of 25,000 ounces. Calculate the total number of animals on the farm.'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"First, let's find out how many horses there are on the farm. We know that each horse is fed 230 ounces of horse food per day and the total amount of horse food needed per day is 12880 ounces. \\n\\nTo find the number of horses, we divide the total amount of horse food by the amount of food each horse consumes:\\n\\nNumber of horses = Total horse food per day / Food per horse per day\\nNumber of horses = 12880 ounces / 230 ounces per horse\\nNumber of horses = 56\\n\\nNow, we know the ratio of sheep to horses is 5 to 7. Since we have 56 horses, we can set up a proportion to find the number of sheep:\\n\\nSheep : Horses = 5 : 7\\nSheep : 56 = 5 : 7\\n\\nTo find the number of sheep, we cross-multiply and solve for the number of sheep:\\n\\nSheep * 7 = 5 * 56\\nSheep = (5 * 56) / 7\\nSheep = 280 / 7\\nSheep = 40\\n\\nSo, there are 40 sheep on the farm.\\n\\nNow, let's calculate the total number of animals on the farm:\\n\\nTotal animals = Number of sheep + Number of horses\\nTotal animals = 40 + 56\\nTotal animals = 96\\n\\nThe farm has a total of 96 animals.\\n\\nNow, let's verify that the total daily food expenditure matches the given information. We know each sheep consumes 150 ounces of sheep food per day, and there are 40 sheep:\\n\\nTotal sheep food per day = Number of sheep * Food per sheep per day\\nTotal sheep food per day = 40 * 150 ounces\\nTotal sheep food per day = 6000 ounces\\n\\nWe already know the total horse food per day is 12880 ounces. Now, let's add the total sheep food to the total horse food to find the total daily food expenditure:\\n\\nTotal daily food expenditure = Total sheep food per day + Total horse food per day\\nTotal daily food expenditure = 6000 ounces + 12880 ounces\\nTotal daily food expenditure = 18880 ounces\\n\\nHowever, this does not match the given total daily food expenditure of 25,000 ounces. It seems there might be an error in the given information or in the calculations. Please double-check the information provided or the calculations.\"}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sft_ds['test'][11]['messages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2461911d-ad6e-4556-a90e-0d89a8325d7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'Solve the given high school math problem by providing a clear explanation of each step leading to the final solution.\\n\\n    Provide a detailed breakdown of your calculations, beginning with an explanation of the problem and describing how you derive each formula, value, or conclusion. Use logical steps that build upon one another, to arrive at the final answer in a systematic manner.\\n\\n    # Steps\\n\\n    1. **Understand the Problem**: Restate the given math problem and clearly identify the main question and any important given values.\\n    2. **Set Up**: Identify the key formulas or concepts that could help solve the problem (e.g., algebraic manipulation, geometry formulas, trigonometric identities).\\n    3. **Solve Step-by-Step**: Iteratively progress through each step of the math problem, justifying why each consecutive operation brings you closer to the solution.\\n    4. **Double Check**: If applicable, double check the work for accuracy and sense, and mention potential alternative approaches if any.\\n    5. **Final Answer**: Provide the numerical or algebraic solution clearly, accompanied by appropriate units if relevant.\\n\\n    # Notes\\n\\n    - Always clearly define any variable or term used.\\n    - Wherever applicable, include unit conversions or context to explain why each formula or step has been chosen.\\n    - Assume the level of mathematics is suitable for high school, and avoid overly advanced math techniques unless they are common at that level.\\n    '},\n",
       " {'role': 'user',\n",
       "  'content': 'On Stewar Farm, the ratio between the number of sheep and the number of horses is 5 to 7. If each horse is fed 230 ounces of horse food per day and the farm needs a total of 12880 ounces of horse food per day, find the number of sheep on the farm. Additionally, each sheep consumes 150 ounces of sheep food per day, and the farm has a total daily food expenditure of 25,000 ounces. Calculate the total number of animals on the farm.'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"First, let's find out how many horses there are on the farm. We know that each horse is fed 230 ounces of horse food per day and the total amount of horse food needed per day is 12880 ounces. \\n\\nTo find the number of horses, we divide the total amount of horse food by the amount of food each horse consumes:\\n\\nNumber of horses = Total horse food per day / Food per horse per day\\nNumber of horses = 12880 ounces / 230 ounces per horse\\nNumber of horses = 56\\n\\nNow, we know the ratio of sheep to horses is 5 to 7. Since we have 56 horses, we can set up a proportion to find the number of sheep:\\n\\nSheep : Horses = 5 : 7\\nSheep : 56 = 5 : 7\\n\\nTo find the number of sheep, we cross-multiply and solve for the number of sheep:\\n\\nSheep * 7 = 5 * 56\\nSheep = (5 * 56) / 7\\nSheep = 280 / 7\\nSheep = 40\\n\\nSo, there are 40 sheep on the farm.\\n\\nNow, let's calculate the total number of animals on the farm:\\n\\nTotal animals = Number of sheep + Number of horses\\nTotal animals = 40 + 56\\nTotal animals = 96\\n\\nThe farm has a total of 96 animals.\\n\\nNow, let's verify that the total daily food expenditure matches the given information. We know each sheep consumes 150 ounces of sheep food per day, and there are 40 sheep:\\n\\nTotal sheep food per day = Number of sheep * Food per sheep per day\\nTotal sheep food per day = 40 * 150 ounces\\nTotal sheep food per day = 6000 ounces\\n\\nWe already know the total horse food per day is 12880 ounces. Now, let's add the total sheep food to the total horse food to find the total daily food expenditure:\\n\\nTotal daily food expenditure = Total sheep food per day + Total horse food per day\\nTotal daily food expenditure = 6000 ounces + 12880 ounces\\nTotal daily food expenditure = 18880 ounces\\n\\nHowever, this does not match the given total daily food expenditure of 25,000 ounces. It seems there might be an error in the given information or in the calculations. Please double-check the information provided or the calculations.\"}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sft_ds['test'][11]['messages']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dfc7db-7107-4615-a447-9ae280313f23",
   "metadata": {},
   "source": [
    "# Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d862ddb-c7e1-40b9-a40e-26d5d32de1f2",
   "metadata": {},
   "source": [
    "## Quantization Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed01b0d6-ca52-40b3-82a8-906a10b24747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BitsAndBytesConfig {\n",
       "  \"_load_in_4bit\": true,\n",
       "  \"_load_in_8bit\": false,\n",
       "  \"bnb_4bit_compute_dtype\": \"float16\",\n",
       "  \"bnb_4bit_quant_storage\": \"uint8\",\n",
       "  \"bnb_4bit_quant_type\": \"nf4\",\n",
       "  \"bnb_4bit_use_double_quant\": true,\n",
       "  \"llm_int8_enable_fp32_cpu_offload\": false,\n",
       "  \"llm_int8_has_fp16_weight\": false,\n",
       "  \"llm_int8_skip_modules\": null,\n",
       "  \"llm_int8_threshold\": 6.0,\n",
       "  \"load_in_4bit\": true,\n",
       "  \"load_in_8bit\": false,\n",
       "  \"quant_method\": \"bitsandbytes\"\n",
       "}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) Prepare 4-bit quantization config\n",
    "bnb_config = setup_quantization_4bit()\n",
    "bnb_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92433eb-01a3-44f6-af56-37caf17d1095",
   "metadata": {},
   "source": [
    "## Model and Tokenizer Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c7a3e8c-027f-49a3-b203-7d96cd022fd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-21): 22 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=5632, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2) Load model & tokenizer\n",
    "model, tokenizer = load_model_and_tokenizer(MODEL_NAME, bnb_config)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc3b8b3-c7dd-4e0d-9372-d6b71c2942d3",
   "metadata": {},
   "source": [
    "## LoRA Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a9252e7-d452-46d8-8de1-c66299c1e06e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Lora Modules:  ['^lm_head.weight$', '^model.embed_tokens.weight$', 'model.layers.21.mlp.down_proj', 'model.layers.1.mlp.down_proj', 'model.layers.2.mlp.gate_proj', 'model.layers.5.mlp.gate_proj', 'model.layers.7.mlp.up_proj', 'model.layers.3.mlp.up_proj', 'model.layers.12.self_attn.k_proj', 'model.layers.21.self_attn.k_proj', 'model.layers.13.self_attn.o_proj', 'model.layers.15.self_attn.o_proj', 'model.layers.0.self_attn.q_proj', 'model.layers.14.self_attn.q_proj', 'model.layers.1.self_attn.v_proj', 'model.layers.2.self_attn.v_proj']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/electron/PycharmProjects/fine_tune_llm/venv/lib/python3.12/site-packages/peft/mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(32000, 2048)\n",
       "        (layers): ModuleList(\n",
       "          (0): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "              (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "              (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=5632, out_features=2048, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          )\n",
       "          (1): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "              (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5632, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5632, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          )\n",
       "          (2): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "              (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5632, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=5632, out_features=2048, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          )\n",
       "          (3): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "              (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "              (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "              (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5632, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): Linear4bit(in_features=5632, out_features=2048, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          )\n",
       "          (4): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "              (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "              (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "              (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=5632, out_features=2048, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          )\n",
       "          (5): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "              (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "              (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "              (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5632, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=5632, out_features=2048, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          )\n",
       "          (6): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "              (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "              (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "              (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=5632, out_features=2048, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          )\n",
       "          (7): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "              (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "              (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "              (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=5632, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): Linear4bit(in_features=5632, out_features=2048, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          )\n",
       "          (8-11): 4 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "              (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "              (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "              (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=5632, out_features=2048, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          )\n",
       "          (12): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "              (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=5632, out_features=2048, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          )\n",
       "          (13): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "              (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "              (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=5632, out_features=2048, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          )\n",
       "          (14): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "              (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "              (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=5632, out_features=2048, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          )\n",
       "          (15): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "              (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "              (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=5632, out_features=2048, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          )\n",
       "          (16-20): 5 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "              (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "              (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "              (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=5632, out_features=2048, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          )\n",
       "          (21): LlamaDecoderLayer(\n",
       "            (self_attn): LlamaAttention(\n",
       "              (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
       "              (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=2048, out_features=5632, bias=False)\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=5632, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5632, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3) Apply LoRA adapters\n",
    "model = apply_lora_peft(model, spectrum=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2ba39e-a2a9-44b7-9204-d1b6484a027a",
   "metadata": {},
   "source": [
    "## SFT Config Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f20c8143-ded8-49c9-9ad3-2eaa4afb8e94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SFTConfig(output_dir='sft_tinnyllama', overwrite_output_dir=False, do_train=False, do_eval=True, do_predict=False, eval_strategy=<IntervalStrategy.STEPS: 'steps'>, prediction_loss_only=False, per_device_train_batch_size=2, per_device_eval_batch_size=2, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=2, eval_accumulation_steps=None, eval_delay=0, torch_empty_cache_steps=None, learning_rate=0.0001, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1, max_steps=-1, lr_scheduler_type=<SchedulerType.COSINE: 'cosine'>, lr_scheduler_kwargs={}, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='sft_tinnyllama/runs/May15_22-11-34_electrion-legion', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=20, logging_nan_inf_filter=True, save_strategy=<SaveStrategy.STEPS: 'steps'>, save_steps=200, save_total_limit=3, save_safetensors=True, save_on_each_node=False, save_only_model=False, restore_callback_states_from_checkpoint=False, no_cuda=False, use_cpu=False, use_mps_device=False, seed=42, data_seed=None, jit_mode_eval=False, use_ipex=False, bf16=False, fp16=True, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=200, dataloader_num_workers=0, dataloader_prefetch_factor=None, past_index=-1, run_name='tinyllama-sft', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=True, metric_for_best_model='loss', greater_is_better=False, ignore_data_skip=False, fsdp=[], fsdp_min_num_params=0, fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False}, tp_size=0, fsdp_transformer_layer_cls_to_wrap=None, accelerator_config=AcceleratorConfig(split_batches=False, dispatch_batches=None, even_batches=True, use_seedable_sampler=True, non_blocking=False, gradient_accumulation_kwargs=None, use_configured_state=False), deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=False, length_column_name='length', report_to=['wandb'], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, dataloader_persistent_workers=False, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=None, hub_always_push=False, gradient_checkpointing=False, gradient_checkpointing_kwargs=None, include_inputs_for_metrics=False, include_for_metrics=[], eval_do_concat_batches=True, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, include_tokens_per_second=False, include_num_input_tokens_seen=False, neftune_noise_alpha=None, optim_target_modules=None, batch_eval_metrics=False, eval_on_start=False, use_liger_kernel=False, eval_use_gather_object=False, average_tokens_across_devices=False, model_init_kwargs=None, dataset_text_field='text', dataset_kwargs=None, dataset_num_proc=None, eos_token=None, pad_token=None, max_length=1024, packing=True, padding_free=False, eval_packing=None, completion_only_loss=None, dataset_batch_size=None, num_of_sequences=None, chars_per_token=None, max_seq_length=None, use_liger=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4) Build training arguments\n",
    "training_args = make_training_args()\n",
    "training_args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db7549f-9309-4102-96b1-5ddc68f897f3",
   "metadata": {},
   "source": [
    "# Trainer Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef9ee88f-a328-4155-83b1-c6e29b570767",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting train dataset to ChatML: 100%|‚ñà| 10000/10000 [00:00<00:00, 12167.15 e\n",
      "Applying chat template to train dataset: 100%|‚ñà| 10000/10000 [00:00<00:00, 15337\n",
      "Tokenizing train dataset: 100%|‚ñà‚ñà| 10000/10000 [00:07<00:00, 1337.98 examples/s]\n",
      "Packing train dataset: 100%|‚ñà‚ñà‚ñà| 10000/10000 [00:00<00:00, 589650.79 examples/s]\n",
      "Converting eval dataset to ChatML: 100%|‚ñà| 1000/1000 [00:00<00:00, 12351.59 exam\n",
      "Applying chat template to eval dataset: 100%|‚ñà| 1000/1000 [00:00<00:00, 15220.74\n",
      "Tokenizing eval dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 1480.61 examples/s]\n",
      "Packing eval dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 340917.17 examples/s]\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<trl.trainer.sft_trainer.SFTTrainer at 0x70c34c414470>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5) Instantiate trainer\n",
    "trainer = build_trainer(model, tokenizer, sft_ds['train'], sft_ds['eval'], training_args)\n",
    "trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "50a51500-d22e-4dd1-9546-5fbe10ad0d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mszamani\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/electron/PycharmProjects/fine_tune_llm/foundational/wandb/run-20250514_180909-7ttq1mt7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/szamani/lora_fine_tune_math_question_answer/runs/7ttq1mt7' target=\"_blank\">tinyllama-sft</a></strong> to <a href='https://wandb.ai/szamani/lora_fine_tune_math_question_answer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/szamani/lora_fine_tune_math_question_answer' target=\"_blank\">https://wandb.ai/szamani/lora_fine_tune_math_question_answer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/szamani/lora_fine_tune_math_question_answer/runs/7ttq1mt7' target=\"_blank\">https://wandb.ai/szamani/lora_fine_tune_math_question_answer/runs/7ttq1mt7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1796' max='1796' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1796/1796 38:56, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.562600</td>\n",
       "      <td>0.548954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.504800</td>\n",
       "      <td>0.521572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.496400</td>\n",
       "      <td>0.508515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.494100</td>\n",
       "      <td>0.500428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.463400</td>\n",
       "      <td>0.494781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.501100</td>\n",
       "      <td>0.491200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.499600</td>\n",
       "      <td>0.489170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.482600</td>\n",
       "      <td>0.488456</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1796, training_loss=0.531722470487412, metrics={'train_runtime': 2338.9691, 'train_samples_per_second': 3.071, 'train_steps_per_second': 0.768, 'total_flos': 4.575488604163277e+16, 'train_loss': 0.531722470487412})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6) Kick off training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8aa8683f-dcda-4363-a3d5-3f2e37b9f4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Save adapter weights & final state\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b35b7795-75eb-43fe-a15e-26ff2f1e26ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(os.path.join(training_args.output_dir, \"adapter_model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d6043f88-bd61-4f04-92e2-992de7914e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 2,252,800 || all params: 1,102,301,184 || trainable%: 0.2044\n"
     ]
    }
   ],
   "source": [
    "trainer.model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da896b95-5f13-4662-9e41-ccf2f5307c03",
   "metadata": {},
   "source": [
    "# Full Dump of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5413c4-7ee9-47a3-ace5-ec138d4b6348",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_merged_model_for_serving(\n",
    "    base_model_name: str,\n",
    "    adapter_dir: str,\n",
    "    export_dir: str,\n",
    "    tokenizer,\n",
    "    bnb_config=None,  # optional quant config if your base is quantized\n",
    "    device_map=\"auto\",\n",
    "):\n",
    "    # 1) Load the base model in the same setup used for training\n",
    "    if bnb_config:\n",
    "        base = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_name,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=device_map,\n",
    "        )\n",
    "    else:\n",
    "        base = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_name,\n",
    "            device_map=device_map,\n",
    "        )\n",
    "\n",
    "    # 2) Wrap and load adapters\n",
    "    peft_model = PeftModel.from_pretrained(\n",
    "        base,\n",
    "        adapter_dir,\n",
    "        device_map=device_map,\n",
    "    )\n",
    "\n",
    "    # 3) Merge LoRA weights & unload adapter wrapper\n",
    "    merged_model = peft_model.merge_and_unload()\n",
    "    merged_model.config.use_cache = True  # set for inference\n",
    "\n",
    "    # 4) Save the merged model + config.json + tokenizer\n",
    "    merged_model.save_pretrained(export_dir)\n",
    "    tokenizer.save_pretrained(export_dir)\n",
    "\n",
    "    print(f\"Full model saved to {export_dir}. You can now run:\")\n",
    "    print(f\"  vllm serve {export_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cad260-2807-4e34-9d7b-cf61481b24bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_dir = os.path.join(training_args.output_dir, \"merged_model_for_serving\")\n",
    "export_merged_model_for_serving(\n",
    "    base_model_name=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    adapter_dir=os.path.join(training_args.output_dir, \"adapter_model\"),\n",
    "    export_dir=export_dir,\n",
    "    tokenizer=tokenizer,\n",
    "    bnb_config=bnb_config,            # if you used 4-bit quantization\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8139a086-e423-4625-a5f9-94175847bee2",
   "metadata": {},
   "source": [
    "# Evaluation Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7320381a-1919-4776-a59c-94892281abc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function run_inference.<locals>.gen_batch at 0x72f20c14fc40> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [10:35<00:00,  1.57 examples/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.0,\n",
       " 'bleu': 0.0007939185784332719,\n",
       " 'rouge1': np.float64(0.11707655447961005),\n",
       " 'rouge2': np.float64(0.05219114355057252),\n",
       " 'rougeL': np.float64(0.0938309527964162),\n",
       " 'bertscore_precision': 0.792242191016674,\n",
       " 'bertscore_recall': 0.7308731575012207,\n",
       " 'bertscore_f1': 0.7594701766371726,\n",
       " 'avg_levenshtein': 797.449}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fresh_preds = run_inference(model, tokenizer, sft_ds['test'])\n",
    "fresh_metrics = evaluate_generation(fresh_preds)\n",
    "fresh_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdd5a9c-2879-4b5c-95ca-a4bea197bd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fresh_preds['messages'][31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86653de-014a-47ce-8420-0bbe9f1fc9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "fresh_preds['prediction'][31]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ddc733-426e-4eea-92eb-a10c148816a8",
   "metadata": {},
   "source": [
    "# Reload Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33139575-3a65-4ef6-9df1-d22a0ddbc745",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trained_or_base(\n",
    "    model_name: str,\n",
    "    adapter_path: str,\n",
    "    load_finetuned: bool = True,\n",
    "    device_map: str = \"auto\",\n",
    "):\n",
    "    # 1) Prepare quant config and load base model & tokenizer\n",
    "    bnb_config = setup_quantization_4bit()\n",
    "    model, tokenizer = load_model_and_tokenizer(model_name, bnb_config, device_map=device_map)\n",
    "\n",
    "    if load_finetuned:\n",
    "        # wrap base in PEFT and load saved adapter weights\n",
    "        model = apply_lora_peft(model)\n",
    "        model = PeftModel.from_pretrained(model, adapter_path, device_map=device_map)\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cf4717-fc23-4b7f-8733-b672420aa36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_base_model_non_quantized(\n",
    "    model_name: str,\n",
    "    device_map: str = \"auto\",\n",
    "):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.add_special_tokens({\"pad_token\": tokenizer.eos_token})\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, device_map=device_map)\n",
    "    model.config.use_cache = True\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa300da9-cf0e-4348-a482-475a977eb54f",
   "metadata": {},
   "source": [
    "# Evaluation After Reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "40a0ff5e-b554-4f52-a695-1834d79444a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [2:00:44<00:00,  7.24s/ examples]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.0,\n",
       " 'bleu': 0.03395986881810831,\n",
       " 'rouge1': np.float64(0.22268675405019422),\n",
       " 'rouge2': np.float64(0.09641640587021322),\n",
       " 'rougeL': np.float64(0.16172563862058176),\n",
       " 'bertscore_precision': 0.7793596202731132,\n",
       " 'bertscore_recall': 0.7538000769615173,\n",
       " 'bertscore_f1': 0.7653101402521133,\n",
       " 'avg_levenshtein': 785.509}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1) Off-the-shelf Non-quantized base\n",
    "base_nq_model, base_nq_tokenizer = load_base_model_non_quantized(MODEL_NAME)\n",
    "base_nq_preds = run_inference(base_nq_model, base_nq_tokenizer, sft_ds['test'])\n",
    "base_nq_metrics = evaluate_generation(base_nq_preds)\n",
    "base_nq_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3f6f2062-2fd1-4648-9cc9-9a2e9cf16914",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['messages', 'prediction'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_nq_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3eea9718-ab49-4363-8549-5c9f284f9252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'Solve the given high school math problem by providing a clear explanation of each step leading to the final solution.\\n\\n    Provide a detailed breakdown of your calculations, beginning with an explanation of the problem and describing how you derive each formula, value, or conclusion. Use logical steps that build upon one another, to arrive at the final answer in a systematic manner.\\n\\n    # Steps\\n\\n    1. **Understand the Problem**: Restate the given math problem and clearly identify the main question and any important given values.\\n    2. **Set Up**: Identify the key formulas or concepts that could help solve the problem (e.g., algebraic manipulation, geometry formulas, trigonometric identities).\\n    3. **Solve Step-by-Step**: Iteratively progress through each step of the math problem, justifying why each consecutive operation brings you closer to the solution.\\n    4. **Double Check**: If applicable, double check the work for accuracy and sense, and mention potential alternative approaches if any.\\n    5. **Final Answer**: Provide the numerical or algebraic solution clearly, accompanied by appropriate units if relevant.\\n\\n    # Notes\\n\\n    - Always clearly define any variable or term used.\\n    - Wherever applicable, include unit conversions or context to explain why each formula or step has been chosen.\\n    - Assume the level of mathematics is suitable for high school, and avoid overly advanced math techniques unless they are common at that level.\\n    '},\n",
       " {'role': 'user',\n",
       "  'content': 'There are three numbers: 10, 11 and 12. What is the product of the largest number and the second largest number?'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'The largest number is 12 and the second largest number is 11. \\n\\nThe product of the largest number and the second largest number is: \\n12 * 11 = 132'}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_nq_preds['messages'][31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2b5f84eb-0558-47b5-9f34-e30e8f783b20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'numerical answer: 12\\n\\n    - Use clear and concise language, avoiding jargon or technical terms that may confuse the reader.\\n    - Provide examples or visual aids to help illustrate the steps and formulas.\\n    - Use a clear and readable font size and style, and avoid using too many lines or paragraphs.\\n    - Use bullet points or numbered lists to organize the steps and formulas.\\n    - Use a consistent formatting style, such as bold or italicized text, to make the solution easy to read.\\n    - Provide a clear and concise explanation of each step, including any necessary assumptions or limitations.\\n    - Use a consistent format for the solution, such as using a decimal point or rounding to the nearest integer.\\n    - Use a consistent format for the answer, such as using a decimal point or rounding to the nearest integer.\\n    - Use a consistent format for the solution, such as using a decimal point or rounding to the nearest integer.\\n    - Use a consistent format for the answer, such as using a decimal point or rounding to the nearest integer.\\n    - Use a consistent format for the solution, such as using a decimal'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_nq_preds['prediction'][31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7429e169-20c4-47e3-99b8-3097fae4a7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [10:53<00:00,  1.53 examples/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.0,\n",
       " 'bleu': 0.032405014898393716,\n",
       " 'rouge1': np.float64(0.19743619803845586),\n",
       " 'rouge2': np.float64(0.08459908887520971),\n",
       " 'rougeL': np.float64(0.1479884543575836),\n",
       " 'bertscore_precision': 0.7803523952960968,\n",
       " 'bertscore_recall': 0.7545850667953491,\n",
       " 'bertscore_f1': 0.7661809774637223,\n",
       " 'avg_levenshtein': 821.885}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2) Off-the-shelf but quantized\n",
    "base_model, base_tokenizer = load_trained_or_base(MODEL_NAME, adapter_path='', load_finetuned=False)\n",
    "base_preds = run_inference(base_model, base_tokenizer, sft_ds['test'])\n",
    "base_metrics = evaluate_generation(base_preds)\n",
    "base_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c251aa7b-fa4f-4185-bf8c-9d9b9a052535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'Solve the given high school math problem by providing a clear explanation of each step leading to the final solution.\\n\\n    Provide a detailed breakdown of your calculations, beginning with an explanation of the problem and describing how you derive each formula, value, or conclusion. Use logical steps that build upon one another, to arrive at the final answer in a systematic manner.\\n\\n    # Steps\\n\\n    1. **Understand the Problem**: Restate the given math problem and clearly identify the main question and any important given values.\\n    2. **Set Up**: Identify the key formulas or concepts that could help solve the problem (e.g., algebraic manipulation, geometry formulas, trigonometric identities).\\n    3. **Solve Step-by-Step**: Iteratively progress through each step of the math problem, justifying why each consecutive operation brings you closer to the solution.\\n    4. **Double Check**: If applicable, double check the work for accuracy and sense, and mention potential alternative approaches if any.\\n    5. **Final Answer**: Provide the numerical or algebraic solution clearly, accompanied by appropriate units if relevant.\\n\\n    # Notes\\n\\n    - Always clearly define any variable or term used.\\n    - Wherever applicable, include unit conversions or context to explain why each formula or step has been chosen.\\n    - Assume the level of mathematics is suitable for high school, and avoid overly advanced math techniques unless they are common at that level.\\n    '},\n",
       " {'role': 'user',\n",
       "  'content': 'There are three numbers: 10, 11 and 12. What is the product of the largest number and the second largest number?'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'The largest number is 12 and the second largest number is 11. \\n\\nThe product of the largest number and the second largest number is: \\n12 * 11 = 132'}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_preds['messages'][31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e4f45fc6-7835-46c1-b3b8-dbc39184edb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ematics:\\n    - Solve the problem by using the given math problem as a starting point.\\n    - Use the given math problem as a guide to help you understand the problem better.\\n    - Use the given math problem as a reference to check your work.\\n    - Use the given math problem as a starting point to guide your own solution.\\n    - Use the given math problem as a reference to check your own work.\\n    - Use the given math problem as a starting point to guide your own solution.\\n    - Use the given math problem as a reference to check your own work.\\n    - Use the given math problem as a starting point to guide your own solution.\\n    - Use the given math problem as a reference to check your own work.\\n    - Use the given math problem as a starting point to guide your own solution.\\n    - Use the given math problem as a reference to check your own work.\\n    - Use the given math problem as a starting point to guide your own solution.\\n    - Use the given math problem as a reference to check your own work.\\n    - Use the given math problem as a starting point to guide your own solution.\\n    - Use the given math'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_preds['prediction'][31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "05cf9fcb-6065-4306-a298-bbe7f533ddb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/electron/PycharmProjects/fine_tune_llm/venv/lib/python3.12/site-packages/peft/tuners/tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "/home/electron/PycharmProjects/fine_tune_llm/venv/lib/python3.12/site-packages/peft/peft_model.py:569: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'base_model.model.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'base_model.model.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight'].\n",
      "  warnings.warn(warn_message)\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [11:15<00:00,  1.48 examples/s]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n",
      "Warning: Empty candidate sentence detected; setting raw BERTscores to 0.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.0,\n",
       " 'bleu': 0.032405014898393716,\n",
       " 'rouge1': np.float64(0.19743619803845586),\n",
       " 'rouge2': np.float64(0.08459908887520971),\n",
       " 'rougeL': np.float64(0.1479884543575836),\n",
       " 'bertscore_precision': 0.7803523952960968,\n",
       " 'bertscore_recall': 0.7545850667953491,\n",
       " 'bertscore_f1': 0.7661809774637223,\n",
       " 'avg_levenshtein': 821.885}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ADAPTER_DIR = \"./sft_tinnyllama/adapter_model\"\n",
    "\n",
    "# 3) Fine-tuned\n",
    "ft_model, ft_tokenizer = load_trained_or_base(MODEL_NAME, ADAPTER_DIR, load_finetuned=True)\n",
    "ft_preds = run_inference(ft_model, ft_tokenizer, sft_ds['test'])\n",
    "ft_metrics = evaluate_generation(ft_preds)\n",
    "ft_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a94658da-f24a-4c67-96f2-8a793012520c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'Solve the given high school math problem by providing a clear explanation of each step leading to the final solution.\\n\\n    Provide a detailed breakdown of your calculations, beginning with an explanation of the problem and describing how you derive each formula, value, or conclusion. Use logical steps that build upon one another, to arrive at the final answer in a systematic manner.\\n\\n    # Steps\\n\\n    1. **Understand the Problem**: Restate the given math problem and clearly identify the main question and any important given values.\\n    2. **Set Up**: Identify the key formulas or concepts that could help solve the problem (e.g., algebraic manipulation, geometry formulas, trigonometric identities).\\n    3. **Solve Step-by-Step**: Iteratively progress through each step of the math problem, justifying why each consecutive operation brings you closer to the solution.\\n    4. **Double Check**: If applicable, double check the work for accuracy and sense, and mention potential alternative approaches if any.\\n    5. **Final Answer**: Provide the numerical or algebraic solution clearly, accompanied by appropriate units if relevant.\\n\\n    # Notes\\n\\n    - Always clearly define any variable or term used.\\n    - Wherever applicable, include unit conversions or context to explain why each formula or step has been chosen.\\n    - Assume the level of mathematics is suitable for high school, and avoid overly advanced math techniques unless they are common at that level.\\n    '},\n",
       " {'role': 'user',\n",
       "  'content': 'There are three numbers: 10, 11 and 12. What is the product of the largest number and the second largest number?'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'The largest number is 12 and the second largest number is 11. \\n\\nThe product of the largest number and the second largest number is: \\n12 * 11 = 132'}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_preds['messages'][31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "84658335-09d2-44c6-b9dc-cf029967383e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ematics:\\n    - Solve the problem by using the given math problem as a starting point.\\n    - Use the given math problem as a guide to help you understand the problem better.\\n    - Use the given math problem as a reference to check your work.\\n    - Use the given math problem as a starting point to guide your own solution.\\n    - Use the given math problem as a reference to check your own work.\\n    - Use the given math problem as a starting point to guide your own solution.\\n    - Use the given math problem as a reference to check your own work.\\n    - Use the given math problem as a starting point to guide your own solution.\\n    - Use the given math problem as a reference to check your own work.\\n    - Use the given math problem as a starting point to guide your own solution.\\n    - Use the given math problem as a reference to check your own work.\\n    - Use the given math problem as a starting point to guide your own solution.\\n    - Use the given math problem as a reference to check your own work.\\n    - Use the given math problem as a starting point to guide your own solution.\\n    - Use the given math'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft_preds['prediction'][31]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef9ba6d-6fbd-41b7-bc3b-eea8854ac639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Print comparison\n",
    "print(\"=== Non-Quantized Base Metrics ===\")\n",
    "for k, v in base_nq_metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "print(\"\\n=== Quantized Base Metrics ===\")\n",
    "for k, v in base_q_metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\")\n",
    "\n",
    "print(\"\\n=== Fine-Tuned Quantized Metrics ===\")\n",
    "for k, v in ft_metrics.items():\n",
    "    print(f\"{k}: {v:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c297294a-39f1-4b39-81da-612c4b5690f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Combined report\n",
    "report = {\n",
    "    k: {\n",
    "        \"base_non_quantized\": base_nq_metrics[k],\n",
    "        \"base_quantized\": base_q_metrics[k],\n",
    "        \"fine_tuned\": ft_metrics[k],\n",
    "    }\n",
    "    for k in base_nq_metrics\n",
    "}\n",
    "df = pd.DataFrame(report).T\n",
    "print(\"\\n=== All Models Comparison ===\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3304da-6333-4c1f-aab6-1083ef37c36d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
